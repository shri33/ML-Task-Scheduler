# =============================================================================
# ML Task Scheduler - Production Docker Compose
# =============================================================================
# Usage:
#   docker-compose -f docker-compose.production.yml --env-file .env.production up -d
#
# Scale API nodes:
#   docker-compose -f docker-compose.production.yml up -d --scale api=3
# =============================================================================

services:
  # ===========================================================================
  # REVERSE PROXY / LOAD BALANCER
  # ===========================================================================
  nginx:
    image: nginx:alpine
    container_name: ml-scheduler-nginx
    restart: always
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./infra/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./infra/nginx/conf.d:/etc/nginx/conf.d:ro
      - ./infra/certs:/etc/nginx/certs:ro
    depends_on:
      - api
      - frontend
    networks:
      - public
      - internal
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 128M

  # ===========================================================================
  # API SERVERS (Horizontally Scalable)
  # ===========================================================================
  api:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    restart: always
    expose:
      - "3001"
    environment:
      NODE_ENV: production
      PORT: 3001
      DATABASE_URL: ${DATABASE_URL}
      REDIS_URL: redis://redis:6379
      JWT_SECRET: ${JWT_SECRET}
      CORS_ORIGIN: ${CORS_ORIGIN}
      ML_SERVICE_URL: http://ml-worker:5001
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      # Swagger protection
      SWAGGER_USER: ${SWAGGER_USER}
      SWAGGER_PASS: ${SWAGGER_PASS}
      # BullMQ Queue
      QUEUE_REDIS_URL: redis://redis:6379
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - internal
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3001/api/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      mode: replicated
      replicas: 2
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          memory: 256M
      update_config:
        parallelism: 1
        delay: 10s
        order: start-first
      rollback_config:
        parallelism: 1
        delay: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"
        tag: "{{.Name}}/{{.ID}}"

  # ===========================================================================
  # ML PREDICTION WORKERS (Queue-Based)
  # ===========================================================================
  ml-worker:
    build:
      context: ./ml-service
      dockerfile: Dockerfile
    restart: always
    expose:
      - "5001"
    environment:
      FLASK_ENV: production
      PORT: 5001
      REDIS_URL: redis://redis:6379
      WORKER_CONCURRENCY: ${ML_WORKER_CONCURRENCY:-4}
    networks:
      - internal
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:5001/api/health')\""]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    deploy:
      mode: replicated
      replicas: 2
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # ===========================================================================
  # QUEUE WORKER (Node.js - BullMQ Processor)
  # ===========================================================================
  queue-worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    restart: always
    command: ["node", "dist/workers/prediction.worker.js"]
    environment:
      NODE_ENV: production
      DATABASE_URL: ${DATABASE_URL}
      REDIS_URL: redis://redis:6379
      ML_SERVICE_URL: http://ml-worker:5001
      WORKER_CONCURRENCY: ${QUEUE_WORKER_CONCURRENCY:-5}
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      ml-worker:
        condition: service_healthy
    networks:
      - internal
    deploy:
      mode: replicated
      replicas: 2
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # ===========================================================================
  # FRONTEND (Static Build via Nginx)
  # ===========================================================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        VITE_API_URL: ${VITE_API_URL:-/api}
    restart: always
    expose:
      - "80"
    networks:
      - internal
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 128M

  # ===========================================================================
  # DATABASE (PostgreSQL)
  # ===========================================================================
  db:
    image: postgres:15-alpine
    restart: always
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./infra/postgres/init:/docker-entrypoint-initdb.d:ro
    networks:
      - internal
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER} -d ${DB_NAME}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"
    # DO NOT EXPOSE IN PRODUCTION - use internal network only
    # ports:
    #   - "5432:5432"

  # ===========================================================================
  # REDIS (Cache + Queue + Socket.IO Adapter)
  # ===========================================================================
  redis:
    image: redis:7-alpine
    restart: always
    command: >
      redis-server
      --appendonly yes
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 60
    volumes:
      - redis_data:/data
    networks:
      - internal
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          memory: 256M
    # DO NOT EXPOSE IN PRODUCTION
    # ports:
    #   - "6379:6379"

  # ===========================================================================
  # OBSERVABILITY - PROMETHEUS
  # ===========================================================================
  prometheus:
    image: prom/prometheus:latest
    restart: always
    volumes:
      - ./infra/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./infra/prometheus/alerts:/etc/prometheus/alerts:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    networks:
      - internal
      - monitoring
    expose:
      - "9090"
    deploy:
      resources:
        limits:
          memory: 512M

  # ===========================================================================
  # OBSERVABILITY - GRAFANA
  # ===========================================================================
  grafana:
    image: grafana/grafana:latest
    restart: always
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_SERVER_ROOT_URL: ${GRAFANA_ROOT_URL:-http://localhost:3002}
    volumes:
      - ./infra/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./infra/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana_data:/var/lib/grafana
    networks:
      - internal
      - monitoring
    expose:
      - "3000"
    deploy:
      resources:
        limits:
          memory: 256M

  # ===========================================================================
  # OBSERVABILITY - LOKI (Log Aggregation)
  # ===========================================================================
  loki:
    image: grafana/loki:latest
    restart: always
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./infra/loki/loki-config.yml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    networks:
      - internal
      - monitoring
    expose:
      - "3100"
    deploy:
      resources:
        limits:
          memory: 256M

# =============================================================================
# NETWORKS
# =============================================================================
networks:
  public:
    driver: bridge
  internal:
    driver: bridge
    internal: true
  monitoring:
    driver: bridge
    internal: true

# =============================================================================
# VOLUMES
# =============================================================================
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  loki_data:
    driver: local
