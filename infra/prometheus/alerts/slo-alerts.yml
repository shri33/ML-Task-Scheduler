# =============================================================================
# Prometheus Alerting Rules
# SLO-based alerts for ML Task Scheduler
# =============================================================================

groups:
  # ==========================================================================
  # SLO Alerts - High Priority
  # ==========================================================================
  - name: slo-alerts
    rules:
      # API Availability SLO
      - alert: APIAvailabilitySLOBreach
        expr: |
          (
            1 - (
              sum(rate(http_requests_total{app="api", status=~"5.."}[1h]))
              /
              sum(rate(http_requests_total{app="api"}[1h]))
            )
          ) < 0.999
        for: 5m
        labels:
          severity: critical
          team: platform
          slo: availability
        annotations:
          summary: "API availability below SLO (99.9%)"
          description: "API availability is {{ $value | humanizePercentage }} over the last hour"
          runbook_url: "https://wiki.example.com/runbooks/api-availability"
          dashboard_url: "https://grafana.example.com/d/ml-scheduler-slo"

      # Latency SLO
      - alert: APILatencySLOBreach
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket{app="api"}[5m])) by (le)
          ) > 0.2
        for: 5m
        labels:
          severity: warning
          team: platform
          slo: latency
        annotations:
          summary: "API P95 latency exceeds SLO (200ms)"
          description: "P95 latency is {{ $value | humanizeDuration }}"
          runbook_url: "https://wiki.example.com/runbooks/api-latency"

      # Queue Backlog SLO
      - alert: QueueBacklogSLOBreach
        expr: bullmq_queue_waiting_total{queue="predictions"} > 100
        for: 10m
        labels:
          severity: warning
          team: platform
          slo: queue
        annotations:
          summary: "Queue backlog exceeds SLO (100 jobs)"
          description: "{{ $labels.queue }} queue has {{ $value }} waiting jobs"

      # Error Rate SLO
      - alert: ErrorRateSLOBreach
        expr: |
          (
            sum(rate(http_requests_total{app="api", status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total{app="api"}[5m]))
          ) > 0.01
        for: 5m
        labels:
          severity: critical
          team: platform
          slo: errors
        annotations:
          summary: "Error rate exceeds SLO (1%)"
          description: "Error rate is {{ $value | humanizePercentage }}"

  # ==========================================================================
  # Infrastructure Alerts
  # ==========================================================================
  - name: infrastructure-alerts
    rules:
      # Pod restart alert
      - alert: HighPodRestartRate
        expr: |
          increase(kube_pod_container_status_restarts_total{namespace="ml-scheduler"}[1h]) > 5
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High pod restart rate"
          description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour"

      # CPU pressure
      - alert: HighCPUUsage
        expr: |
          avg(rate(container_cpu_usage_seconds_total{namespace="ml-scheduler"}[5m])) by (pod) > 0.8
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High CPU usage"
          description: "Pod {{ $labels.pod }} CPU usage is {{ $value | humanizePercentage }}"

      # Memory pressure
      - alert: HighMemoryUsage
        expr: |
          (
            container_memory_working_set_bytes{namespace="ml-scheduler"}
            /
            container_spec_memory_limit_bytes{namespace="ml-scheduler"}
          ) > 0.85
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High memory usage"
          description: "Pod {{ $labels.pod }} memory usage is {{ $value | humanizePercentage }}"

      # PVC usage
      - alert: PVCNearlyFull
        expr: |
          (
            kubelet_volume_stats_used_bytes{namespace="ml-scheduler"}
            /
            kubelet_volume_stats_capacity_bytes{namespace="ml-scheduler"}
          ) > 0.85
        for: 15m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "PVC nearly full"
          description: "PVC {{ $labels.persistentvolumeclaim }} is {{ $value | humanizePercentage }} full"

  # ==========================================================================
  # Database Alerts
  # ==========================================================================
  - name: database-alerts
    rules:
      # Slow queries
      - alert: SlowDatabaseQueries
        expr: |
          histogram_quantile(0.95,
            sum(rate(prisma_client_queries_duration_seconds_bucket[5m])) by (le)
          ) > 0.5
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Slow database queries detected"
          description: "P95 query latency is {{ $value | humanizeDuration }}"

      # Connection pool exhaustion
      - alert: DatabaseConnectionPoolExhaustion
        expr: |
          pg_stat_activity_count{state="active"} 
          / 
          pg_settings_max_connections > 0.8
        for: 5m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "Database connection pool near exhaustion"
          description: "{{ $value | humanizePercentage }} of connections in use"

  # ==========================================================================
  # Redis Alerts
  # ==========================================================================
  - name: redis-alerts
    rules:
      # Low cache hit rate
      - alert: LowCacheHitRate
        expr: |
          (
            redis_keyspace_hits_total
            /
            (redis_keyspace_hits_total + redis_keyspace_misses_total)
          ) < 0.8
        for: 15m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Low Redis cache hit rate"
          description: "Cache hit rate is {{ $value | humanizePercentage }}"

      # Memory usage
      - alert: RedisHighMemoryUsage
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Redis high memory usage"
          description: "Redis memory usage is {{ $value | humanizePercentage }}"

  # ==========================================================================
  # ML Service Alerts
  # ==========================================================================
  - name: ml-service-alerts
    rules:
      # Slow predictions
      - alert: SlowMLPredictions
        expr: |
          histogram_quantile(0.95,
            sum(rate(ml_prediction_duration_seconds_bucket[5m])) by (le)
          ) > 5
        for: 10m
        labels:
          severity: warning
          team: ml
        annotations:
          summary: "ML predictions are slow"
          description: "P95 prediction time is {{ $value | humanizeDuration }}"

      # High prediction error rate
      - alert: HighMLErrorRate
        expr: |
          sum(rate(ml_prediction_errors_total[5m]))
          /
          sum(rate(ml_predictions_total[5m])) > 0.05
        for: 5m
        labels:
          severity: critical
          team: ml
        annotations:
          summary: "High ML prediction error rate"
          description: "ML error rate is {{ $value | humanizePercentage }}"

      # ML Worker down
      - alert: MLWorkerDown
        expr: |
          sum(up{app="ml-worker"}) == 0
        for: 2m
        labels:
          severity: critical
          team: ml
        annotations:
          summary: "All ML workers are down"
          description: "No ML workers are responding to health checks"

  # ==========================================================================
  # Queue Alerts
  # ==========================================================================
  - name: queue-alerts
    rules:
      # Queue growing
      - alert: QueueGrowing
        expr: |
          deriv(bullmq_queue_waiting_total{queue="predictions"}[10m]) > 0.5
        for: 15m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Queue is consistently growing"
          description: "Queue {{ $labels.queue }} is growing at {{ $value }} jobs/second"

      # Jobs failing
      - alert: HighJobFailureRate
        expr: |
          sum(rate(bullmq_job_failed_total[5m])) by (queue)
          /
          sum(rate(bullmq_job_completed_total[5m])) by (queue) > 0.1
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High job failure rate"
          description: "Queue {{ $labels.queue }} failure rate is {{ $value | humanizePercentage }}"

      # Queue worker down
      - alert: QueueWorkerDown
        expr: |
          sum(up{app="queue-worker"}) == 0
        for: 2m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "All queue workers are down"
          description: "No queue workers are responding to health checks"
