# =============================================================================
# ML Worker Deployment
# Python-based prediction workers consuming from BullMQ
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-worker
  namespace: ml-scheduler
  labels:
    app: ml-worker
    tier: worker
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ml-worker
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: ml-worker
        tier: worker
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "5001"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: ml-worker-service-account
      containers:
        - name: ml-worker
          image: ghcr.io/your-org/ml-scheduler-ml:latest
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 5001
              protocol: TCP
          envFrom:
            - secretRef:
                name: ml-scheduler-secrets
            - configMapRef:
                name: ml-scheduler-config
          env:
            - name: FLASK_ENV
              value: "production"
            - name: PORT
              value: "5001"
            - name: WORKER_CONCURRENCY
              value: "4"
          resources:
            requests:
              cpu: "500m"
              memory: "1Gi"
            limits:
              cpu: "2000m"
              memory: "2Gi"
          livenessProbe:
            httpGet:
              path: /api/health
              port: http
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /api/health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            allowPrivilegeEscalation: false
          volumeMounts:
            - name: models
              mountPath: /app/models
              readOnly: true
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: ml-models-pvc
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: ml-worker
                topologyKey: kubernetes.io/hostname
---
apiVersion: v1
kind: Service
metadata:
  name: ml-worker
  namespace: ml-scheduler
  labels:
    app: ml-worker
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 5001
      targetPort: 5001
      protocol: TCP
  selector:
    app: ml-worker
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-worker-hpa
  namespace: ml-scheduler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-worker
  minReplicas: 2
  maxReplicas: 8
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 70
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 600
      policies:
        - type: Percent
          value: 25
          periodSeconds: 120
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
